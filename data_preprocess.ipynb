{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f081073b",
   "metadata": {},
   "source": [
    "# Static joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b533d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f0234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r\"data\\CAMELS_IND_Catchments_Streamflow_Sufficient\\attributes_csv\\camels_ind_anth.csv\")\n",
    "df2 = pd.read_csv(r\"data\\CAMELS_IND_Catchments_Streamflow_Sufficient\\attributes_csv\\camels_ind_clim.csv\")\n",
    "df3 = pd.read_csv(r\"data\\CAMELS_IND_Catchments_Streamflow_Sufficient\\attributes_csv\\camels_ind_geol.csv\")\n",
    "df4 = pd.read_csv(r\"data\\CAMELS_IND_Catchments_Streamflow_Sufficient\\attributes_csv\\camels_ind_hydro.csv\")\n",
    "df5 = pd.read_csv(r\"data\\CAMELS_IND_Catchments_Streamflow_Sufficient\\attributes_csv\\camels_ind_land.csv\")\n",
    "df6 = pd.read_csv(r\"data\\CAMELS_IND_Catchments_Streamflow_Sufficient\\attributes_csv\\camels_ind_name.csv\")\n",
    "df7 = pd.read_csv(r\"data\\CAMELS_IND_Catchments_Streamflow_Sufficient\\attributes_csv\\camels_ind_soil.csv\")\n",
    "df8 = pd.read_csv(r\"data\\CAMELS_IND_Catchments_Streamflow_Sufficient\\attributes_csv\\camels_ind_topo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa65d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes on gauge_id to include all gauge IDs\n",
    "df_merged = df1.merge(df2, on='gauge_id', how='outer') \\\n",
    "              .merge(df3, on='gauge_id', how='outer') \\\n",
    "              .merge(df4, on='gauge_id', how='outer') \\\n",
    "              .merge(df5, on='gauge_id', how='outer') \\\n",
    "              .merge(df6, on='gauge_id', how='outer') \\\n",
    "              .merge(df7, on='gauge_id', how='outer') \\\n",
    "              .merge(df8, on='gauge_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc3c11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape: (242, 211)\n",
      "Number of unique gauge IDs: 242\n",
      "Number of columns: 211\n"
     ]
    }
   ],
   "source": [
    "# Check the shape and number of unique gauge IDs\n",
    "print(f\"Merged DataFrame shape: {df_merged.shape}\")\n",
    "print(f\"Number of unique gauge IDs: {df_merged['gauge_id'].nunique()}\")\n",
    "print(f\"Number of columns: {len(df_merged.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f99de0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate columns: []\n"
     ]
    }
   ],
   "source": [
    "# Check for any duplicate columns (shouldn't be any with proper merge)\n",
    "duplicate_cols = df_merged.columns[df_merged.columns.duplicated()].tolist()\n",
    "print(f\"Duplicate columns: {duplicate_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ae77da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any duplicate columns if they exist\n",
    "df_merged = df_merged.loc[:, ~df_merged.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e344d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved as 'static_all_gauges.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the complete merged dataset\n",
    "df_merged.to_csv(\"static_all_gauges.csv\", index=False)\n",
    "print(\"Merged dataset saved as 'static_all_gauges.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d37be6",
   "metadata": {},
   "source": [
    "# Stactic Data Dimn Redn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccda00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c9e875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UMAP DIMENSIONALITY REDUCTION FOR STATIC DATA ===\n",
      "\n",
      "Original static dataset shape: (242, 211)\n",
      "Number of numerical columns: 198\n",
      "Number of categorical columns: 13\n",
      "Encoding categorical variables...\n",
      "Data for UMAP shape: (242, 210)\n",
      "Missing values before cleaning: 3463\n",
      "Missing values after cleaning: 0\n",
      "Standardizing features...\n",
      "Scaled dataset shape: (242, 210)\n"
     ]
    }
   ],
   "source": [
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== UMAP DIMENSIONALITY REDUCTION FOR STATIC DATA ===\\n\")\n",
    "\n",
    "df_static = pd.read_csv(\"gauge_data/static_all_gauges.csv\")\n",
    "print(f\"Original static dataset shape: {df_static.shape}\")\n",
    "\n",
    "numerical_cols = df_static.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Number of numerical columns: {len(numerical_cols)}\")\n",
    "\n",
    "if 'gauge_id' in numerical_cols:\n",
    "    numerical_cols.remove('gauge_id')\n",
    "\n",
    "df_numeric = df_static[numerical_cols].copy()\n",
    "\n",
    "categorical_cols = df_static.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "print(f\"Number of categorical columns: {len(categorical_cols)}\")\n",
    "\n",
    "# Encode categorical variables if they exist\n",
    "if categorical_cols:\n",
    "    print(\"Encoding categorical variables...\")\n",
    "    df_categorical_encoded = df_static[categorical_cols].copy()\n",
    "    \n",
    "    # Use OrdinalEncoder for categorical variables\n",
    "    encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        if col != 'gauge_id':  # Skip gauge_id\n",
    "            encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "            df_categorical_encoded[col] = encoder.fit_transform(df_categorical_encoded[[col]])\n",
    "            encoders[col] = encoder\n",
    "    \n",
    "    # Combine numerical and encoded categorical data\n",
    "    df_for_umap = pd.concat([df_numeric, df_categorical_encoded], axis=1)\n",
    "else:\n",
    "    df_for_umap = df_numeric.copy()\n",
    "\n",
    "print(f\"Data for UMAP shape: {df_for_umap.shape}\")\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"Missing values before cleaning: {df_for_umap.isnull().sum().sum()}\")\n",
    "df_for_umap = df_for_umap.fillna(df_for_umap.median())\n",
    "print(f\"Missing values after cleaning: {df_for_umap.isnull().sum().sum()}\")\n",
    "\n",
    "# Standardize the features\n",
    "print(\"Standardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_for_umap)\n",
    "print(f\"Scaled dataset shape: {df_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38066b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved  UMAP embedding to 'static_umap_40d.csv'\n"
     ]
    }
   ],
   "source": [
    "n_dim = 40\n",
    "\n",
    "# Create UMAP with current dimension\n",
    "umap_model = umap.UMAP(\n",
    "    n_components=n_dim, \n",
    "    random_state=42, \n",
    "    n_neighbors=15, \n",
    "    min_dist=0.1,\n",
    "    n_epochs=1000,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "embedding_nd = umap_model.fit_transform(df_scaled)\n",
    "\n",
    "# Create base dataframe with gauge_id for joining\n",
    "base_df = df_static[['gauge_id']].copy()\n",
    "\n",
    "column_names = [f\"umap_{n_dim}d_dim_{i+1}\" for i in range(n_dim)]\n",
    "embedding_df = pd.DataFrame(embedding_nd, columns=column_names)\n",
    "\n",
    "# Add gauge_id\n",
    "embedding_with_id = pd.concat([base_df, embedding_df], axis=1)\n",
    "\n",
    "embedding_with_id.to_csv(f\"output/static_all_umap_{n_dim}d.csv\", index=False)\n",
    "print(f\"Saved  UMAP embedding to 'static_umap_{n_dim}d.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b2a0f",
   "metadata": {},
   "source": [
    "# Static and Dynamic joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e505e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "dynamic_dir = \"gauge_data/dynamic\"\n",
    "output_dir = \"output/dynamic\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df_static = pd.read_csv(\"output/static_all_umap_40d.csv\")\n",
    "\n",
    "for fname in os.listdir(dynamic_dir):\n",
    "    if fname.endswith(\".csv\"):\n",
    "        gauge_id = int(fname.split(\".\")[0].lstrip(\"0\"))\n",
    "        \n",
    "        df_dyn = pd.read_csv(os.path.join(dynamic_dir, fname))\n",
    "        \n",
    "        if 'date' in df_dyn.columns:\n",
    "            df_dyn = df_dyn.drop('date', axis=1)\n",
    "            \n",
    "        row = df_static[df_static['gauge_id'] == gauge_id].reset_index().loc[0]\n",
    "        n = len(df_dyn)\n",
    "        duplicates = pd.DataFrame([row] * n, index=df_dyn.index)\n",
    "        merged_df = pd.concat([df_dyn, duplicates], axis=1)\n",
    "        \n",
    "        if 'index' in merged_df.columns:\n",
    "            merged_df = merged_df.drop(columns=['index'])\n",
    "        if 'gauge_id' in merged_df.columns:\n",
    "            merged_df = merged_df.drop(columns=['gauge_id'])\n",
    "            \n",
    "        cat = merged_df.select_dtypes(include=['object', 'string']).columns\n",
    "        \n",
    "        for column in cat:\n",
    "            le = OrdinalEncoder()\n",
    "            reshaped = merged_df[[column]]\n",
    "            merged_df[column] = le.fit_transform(reshaped)\n",
    "        merged_df.to_csv(os.path.join(output_dir, f\"merged_df_{gauge_id}.csv\"), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
